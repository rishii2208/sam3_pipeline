{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aafb3d8",
   "metadata": {},
   "source": [
    "**Prerequisites**\n",
    "- Git repository that contains this project (push your local changes somewhere accessible).\n",
    "- Hugging Face access token with permission to pull SAM3 weights.\n",
    "- Kaggle API token (kaggle.json) if you want the notebook to download the fruits dataset automatically, otherwise mount Google Drive and point to an existing copy of `data/fruits`.\n",
    "- Colab runtime set to GPU (Runtime → Change runtime type → Hardware accelerator → GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05735b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Runtime configuration\n",
    "REPO_URL = \"https://github.com/<your-account>/readme.git\"  #@param {type:\"string\"}\n",
    "PROJECT_DIR = \"/content/readme\"  #@param {type:\"string\"}\n",
    "MOUNT_DRIVE = False  #@param {type:\"boolean\"}\n",
    "DRIVE_DATASET_PATH = \"/content/drive/MyDrive/datasets/fruits\"  #@param {type:\"string\"}\n",
    "DOWNLOAD_WITH_KAGGLE = True  #@param {type:\"boolean\"}\n",
    "TRAIN_DEVICE = \"cuda\"  #@param [\"cuda\", \"cpu\"]\n",
    "CALIBRATION_LIMIT = 0  #@param {type:\"integer\"}\n",
    "EVAL_LIMIT = 0  #@param {type:\"integer\"}\n",
    "PIPELINES = [\"p2\", \"p3\"]\n",
    "assert REPO_URL, \"Set REPO_URL to your Git repository URL.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9125568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Enter authentication tokens (input hidden after execution)\n",
    "import getpass\n",
    "HF_TOKEN = getpass.getpass(\"Hugging Face token (leave blank to skip): \" ).strip()\n",
    "print(\"Token captured?\", bool(HF_TOKEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed2f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Optional: mount Google Drive\n",
    "if MOUNT_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print('Skipping Google Drive mount.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119bcf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Clone (or re-clone) the repository\n",
    "import pathlib\n",
    "import shutil\n",
    "project_path = pathlib.Path(PROJECT_DIR)\n",
    "if project_path.exists():\n",
    "    print(f'Removing existing directory: {project_path}')\n",
    "    shutil.rmtree(project_path)\n",
    "!git clone {REPO_URL} {PROJECT_DIR}\n",
    "%cd {PROJECT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5514930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install Python dependencies (includes SAM3 + Triton)\n",
    "%pip install -q -r requirements.txt\n",
    "%pip install -q -e external/sam3[train]\n",
    "%pip install -q triton==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c300b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Authenticate with Hugging Face\n",
    "if HF_TOKEN:\n",
    "    from huggingface_hub import login\n",
    "    login(token=HF_TOKEN, add_to_git_credential=False)\n",
    "else:\n",
    "    print('HF token not provided; assuming checkpoints already cached.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926cc904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Prepare the fruits dataset\n",
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "project_path = Path(PROJECT_DIR)\n",
    "data_root = project_path / 'data' / 'fruits'\n",
    "data_root.mkdir(parents=True, exist_ok=True)\n",
    "if DOWNLOAD_WITH_KAGGLE:\n",
    "    %pip install -q kaggle\n",
    "    kaggle_dir = Path.home() / '.kaggle'\n",
    "    kaggle_dir.mkdir(exist_ok=True)\n",
    "    cred_path = kaggle_dir / 'kaggle.json'\n",
    "    if not cred_path.exists():\n",
    "        from google.colab import files\n",
    "        print('Upload kaggle.json (Kaggle API token) when prompted.')\n",
    "        uploaded = files.upload()\n",
    "        cred_name = next(iter(uploaded))\n",
    "        cred_path.write_bytes(uploaded[cred_name])\n",
    "        !chmod 600 /root/.kaggle/kaggle.json\n",
    "    !kaggle datasets download afsananadia/fruits-images-dataset-object-detection -p {data_root} -o\n",
    "    zip_path = data_root / 'fruits-images-dataset-object-detection.zip'\n",
    "    if zip_path.exists():\n",
    "        with zipfile.ZipFile(zip_path, 'r') as archive:\n",
    "            archive.extractall(data_root)\n",
    "else:\n",
    "    source_path = Path(DRIVE_DATASET_PATH)\n",
    "    if source_path.exists():\n",
    "        print(f'Copying dataset from {source_path} ...')\n",
    "        if data_root.resolve() != source_path.resolve():\n",
    "            shutil.copytree(source_path, data_root, dirs_exist_ok=True)\n",
    "    else:\n",
    "        print('Drive dataset path missing; ensure data/fruits is populated manually.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ebd1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Build train/val/test splits (idempotent)\n",
    "import pathlib\n",
    "split_index = pathlib.Path(PROJECT_DIR) / 'data' / 'fruits' / 'splits.json'\n",
    "if split_index.exists():\n",
    "    print(f'Splits already exist at {split_index}')\n",
    "else:\n",
    "    !python scripts/split_data.py data/fruits --train-ratio 0.7 --val-ratio 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74742bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Train + evaluate pipelines\n",
    "import os\n",
    "import shlex\n",
    "import subprocess\n",
    "cal_limit = CALIBRATION_LIMIT if CALIBRATION_LIMIT > 0 else None\n",
    "eval_limit = EVAL_LIMIT if EVAL_LIMIT > 0 else None\n",
    "env = os.environ.copy()\n",
    "env['PYTHONPATH'] = PROJECT_DIR\n",
    "def run_cmd(cmd: str):\n",
    "    print(f'\\n>>> {cmd}')\n",
    "    subprocess.run(shlex.split(cmd), cwd=PROJECT_DIR, env=env, check=True)\n",
    "for pipeline in PIPELINES:\n",
    "    train_cmd = f'python scripts/train.py --pipeline {pipeline} --device {TRAIN_DEVICE}'\n",
    "    if cal_limit:\n",
    "        train_cmd += f' --limit {cal_limit}'\n",
    "    run_cmd(train_cmd)\n",
    "    eval_cmd = f'python scripts/evaluate.py --pipeline {pipeline} --device {TRAIN_DEVICE} --split val'\n",
    "    if eval_limit:\n",
    "        eval_cmd += f' --limit {eval_limit}'\n",
    "    run_cmd(eval_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83fafa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Inspect generated result artifacts\n",
    "from pathlib import Path\n",
    "results_dir = Path(PROJECT_DIR) / 'results'\n",
    "if not results_dir.exists():\n",
    "    print('No results directory yet.')\n",
    "else:\n",
    "    for artifact in sorted(results_dir.rglob('*.json')):\n",
    "        rel_path = artifact.relative_to(Path(PROJECT_DIR))\n",
    "        print(rel_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
